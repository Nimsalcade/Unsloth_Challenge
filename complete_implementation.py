import json

# Load original notebook
with open('/home/engine/project/Unsloth_Puzzles.ipynb', 'r') as f:
    nb = json.load(f)

# Find cell 35 and update it with complete implementation
cell_35_new_source = [
    'def transformation_function(batch, linear, labels):',
    '    x = linear(batch).float() # Up projection to large space',
    '    from torch.nn import CrossEntropyLoss',
    '    down_projection_function = CrossEntropyLoss(reduction = "mean")',
    '    # Down projection to small space',
    '    loss = down_projection_function(x.view(-1, x.shape[-1]), labels.view(-1))',
    '    return loss',
    '',
    'class MemoryEfficientLinear(torch.autograd.Function):',
    '    @staticmethod',
    '    def forward(ctx, X, linear, labels, forward_function, chunk_size=4096):',
    '        """',
    '        Memory-efficient forward pass that processes large linear projections in chunks.',
    '        ',
    '        Args:',
    '            X: Input tensor [batch_size, hidden_dim]',
    '            linear: Linear layer [hidden_dim, vocab_size] ',
    '            labels: Target labels [batch_size]',
    '            forward_function: Function that processes (X_chunk, linear, labels) -> loss',
    '            chunk_size: Size of chunks to process vocabulary',
    '            ',
    '        Returns:',
    '            loss: Scalar loss value',
    '        """',
    '        batch_size, hidden_dim = X.shape',
    '        vocab_size = linear.weight.shape[0]',
    '        ',
    '        # Save necessary information for backward',
    '        ctx.linear = linear',
    '        ctx.forward_function = forward_function',
    '        ctx.chunk_size = chunk_size',
    '        ctx.vocab_size = vocab_size',
    '        ctx.batch_size = batch_size',
    '        ctx.hidden_dim = hidden_dim',
    '        ',
    '        # Process vocabulary in chunks to avoid materializing full logits',
    '        total_loss = 0.0',
    '        num_chunks = (vocab_size + chunk_size - 1) // chunk_size',
    '        ',
    '        for chunk_idx in range(num_chunks):',
    '            start_idx = chunk_idx * chunk_size',
    '            end_idx = min((chunk_idx + 1) * chunk_size, vocab_size)',
    '            ',
    '            # Create chunk-specific linear layer',
    '            chunk_weight = linear.weight[start_idx:end_idx]  # [chunk_size, hidden_dim]',
    '            chunk_linear = torch.nn.Linear(hidden_dim, end_idx - start_idx, bias=linear.bias is not None)',
    '            chunk_linear.weight.data = chunk_weight',
    '            if linear.bias is not None:',
    '                chunk_linear.bias.data = linear.bias[start_idx:end_idx]',
    '            ',
    '            # Forward pass for this chunk',
    '            chunk_loss = forward_function(X, chunk_linear, labels, start_idx, end_idx)',
    '            total_loss += chunk_loss',
    '            ',
    '            # Clear intermediate tensors to save memory',
    '            del chunk_weight, chunk_linear',
    '        ',
    '        # Save input for backward pass',
    '        ctx.save_for_backward(X)',
    '        return total_loss / num_chunks',
    '',
    '    @staticmethod',
    '    def backward(ctx, dY):',
    '        """',
    '        Memory-efficient backward pass that reconstructs gradients on the fly.',
    '        ',
    '        Args:',
    '            dY: Upstream gradient of loss [1]',
    '            ',
    '        Returns:',
    '            grad_X: Gradient w.r.t. input [batch_size, hidden_dim]',
    '            grad_linear: Gradient w.r.t. linear parameters',
    '            grad_labels: None (labels don\\'t need gradients)',
    '            grad_forward_function: None (function doesn\\'t need gradients)',
    '        """',
    '        X, = ctx.saved_tensors',
    '        linear = ctx.linear',
    '        forward_function = ctx.forward_function',
    '        chunk_size = ctx.chunk_size',
    '        vocab_size = ctx.vocab_size',
    '        ',
    '        # Initialize gradients',
    '        grad_X = torch.zeros_like(X)',
    '        grad_weight = torch.zeros_like(linear.weight)',
    '        grad_bias = torch.zeros_like(linear.bias) if linear.bias is not None else None',
    '        ',
    '        num_chunks = (vocab_size + chunk_size - 1) // chunk_size',
    '        ',
    '        # Process each chunk to compute gradients',
    '        for chunk_idx in range(num_chunks):',
    '            start_idx = chunk_idx * chunk_size',
    '            end_idx = min((chunk_idx + 1) * chunk_size, vocab_size)',
    '            ',
    '            # Create chunk-specific linear layer',
    '            chunk_weight = linear.weight[start_idx:end_idx]',
    '            chunk_linear = torch.nn.Linear(ctx.hidden_dim, end_idx - start_idx, bias=linear.bias is not None)',
    '            chunk_linear.weight.data = chunk_weight',
    '            if linear.bias is not None:',
    '                chunk_linear.bias.data = linear.bias[start_idx:end_idx]',
    '            ',
    '            # Get gradients for this chunk using autograd',
    '            X_chunk = X.detach().requires_grad_(True)',
    '            chunk_loss = forward_function(X_chunk, chunk_linear, None, start_idx, end_idx)',
    '            ',
    '            # Apply upstream gradient',
    '            chunk_loss = chunk_loss * dY',
    '            ',
    '            # Compute gradients for this chunk',
    '            chunk_loss.backward(retain_graph=True)',
    '            ',
    '            # Accumulate gradients',
    '            grad_X += X_chunk.grad',
    '            grad_weight[start_idx:end_idx] = chunk_linear.weight.grad',
    '            if grad_bias is not None:',
    '                grad_bias[start_idx:end_idx] = chunk_linear.bias.grad',
    '            ',
    '            # Clear intermediate tensors',
    '            del chunk_weight, chunk_linear, X_chunk, chunk_loss',
    '        ',
    '        # Create gradient tuple for linear layer',
    '        grad_linear = (grad_weight, grad_bias)',
    '        ',
    '        return grad_X, grad_linear, None, None'
]

# Update cell 35
nb['cells'][35]['source'] = cell_35_new_source

# Add supporting functions cell (cell 36)
supporting_functions_cell = {
    'cell_type': 'code',
    'execution_count': None,
    'metadata': {},
    'outputs': [],
    'source': [
        '# Supporting functions for MemoryEfficientLinear',
        'import torch.nn.functional as F',
        '',
        'def chunked_cross_entropy_forward(X, linear, labels, start_idx, end_idx):',
        '    """Cross entropy forward function for chunked processing."""',
        '    logits = linear(X)  # [batch_size, chunk_size]',
        '    ',
        '    if labels is not None:',
        '        # Mask labels that are not in this chunk',
        '        mask = (labels >= start_idx) & (labels < end_idx)',
        '        if mask.any():',
        '            chunk_labels = labels[mask] - start_idx  # Adjust to chunk-local indices',
        '            chunk_logits = logits[mask]',
        '            loss = F.cross_entropy(chunk_logits, chunk_labels, reduction=\'mean\')',
        '            # Scale by fraction of labels in this chunk',
        '            return loss * mask.float().mean().item()',
        '        else:',
        '            return torch.tensor(0.0, device=X.device, dtype=X.dtype)',
        '    else:',
        '        # For backward pass - return dummy loss',
        '        return torch.tensor(0.0, device=X.device, dtype=X.dtype)',
        '',
        'def chunked_kl_div_forward(X, linear, labels, start_idx, end_idx):',
        '    """KL Divergence forward function for chunked processing."""',
        '    logits = linear(X)  # [batch_size, chunk_size]',
        '    log_probs = F.log_softmax(logits, dim=-1)',
        '    ',
        '    if labels is not None:',
        '        # For simplicity, assume labels are target distributions',
        '        mask = (labels >= start_idx) & (labels < end_idx)',
        '        if mask.any():',
        '            chunk_labels = labels[mask] - start_idx',
        '            chunk_log_probs = log_probs[mask]',
        '            target_probs = F.one_hot(chunk_labels, num_classes=end_idx-start_idx).float()',
        '            loss = F.kl_div(chunk_log_probs, target_probs, reduction=\'batchmean\')',
        '            return loss * mask.float().mean().item()',
        '        else:',
        '            return torch.tensor(0.0, device=X.device, dtype=X.dtype)',
        '    else:',
        '        return torch.tensor(0.0, device=X.device, dtype=X.dtype)',
        '',
        'def memory_efficient_forward(X, linear, labels, forward_fn, chunk_size=4096):',
        '    """Wrapper for MemoryEfficientLinear forward."""',
        '    return MemoryEfficientLinear.apply(X, linear, labels, forward_fn, chunk_size)',
        '',
        'def vanilla_forward(X, linear, labels, forward_fn):',
        '    """Vanilla forward for comparison."""',
        '    return forward_fn(X, linear, labels, 0, linear.weight.shape[0])'
    ]
}

# Insert cell 36
nb['cells'].insert(36, supporting_functions_cell)

# Save the updated notebook
with open('/home/engine/project/Unsloth_Puzzles.ipynb', 'w') as f:
    json.dump(nb, f, indent=2)

print("Successfully updated cell 35 with MemoryEfficientLinear implementation")
print("Added cell 36 with supporting functions")